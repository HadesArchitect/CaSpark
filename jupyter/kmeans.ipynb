{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: K-Means and DataStax Analytics\n",
    "------\n",
    "<img src=\"images/socialMedia.jpeg\" width=\"400\" height=\"500\">\n",
    "\n",
    "\n",
    "#### Dataset: https://archive.ics.uci.edu/ml/datasets/Facebook+Live+Sellers+in+Thailand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we trying to learn from this dataset? \n",
    "\n",
    "# QUESTION: Can K-Means be used to do social media analysis, can we group together different types of media by the reaction they received?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import cassandra\n",
    "import pyspark\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from random import randint, randrange\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to have nicer formatting of Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper for pretty formatting for Spark DataFrames\n",
    "def showDF(df, limitRows =  5, truncate = True):\n",
    "    if(truncate):\n",
    "        pandas.set_option('display.max_colwidth', 50)\n",
    "    else:\n",
    "        pandas.set_option('display.max_colwidth', -1)\n",
    "    pandas.set_option('display.max_rows', limitRows)\n",
    "    display(df.limit(limitRows).toPandas())\n",
    "    pandas.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dselogo.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tables and Loading Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['dse'])\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Demo Keyspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS accelerate \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set keyspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.set_keyspace('accelerate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table called `socialMedia`. Our PRIMARY will be a unique key (status_id) we generate for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS socialMedia \\\n",
    "                                   (status_id int, social_type text, num_reactions int,\\\n",
    "                                   num_comments int, num_shares int, num_likes int, num_loves int,\\\n",
    "                                   num_wows int, num_hahas int, num_sads int, num_angrys int, \\\n",
    "                                   PRIMARY KEY (status_id))\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do these of these 11 columns represent: \n",
    "\n",
    "* **Status_id**: Unique key created for each row\n",
    "\n",
    "* **Num Reactions**: \n",
    "\n",
    "* **Num Comments**:\n",
    "\n",
    "* **Num Shares**:\n",
    "\n",
    "* **Num Likes**:\n",
    "\n",
    "* **Num Loves**:\n",
    "\n",
    "* **Num Wows**:\n",
    "\n",
    "* **Num Hahas**:\n",
    "\n",
    "* **Num Sads**:\n",
    "\n",
    "* **Num Angrys**:\n",
    "\n",
    "* **Social Type**: Picture or Video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Social Media Dataset\n",
    "<img src=\"images/getTheLikes.png\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from CSV file (socialMedia.csv)\n",
    "\n",
    "#### Insert all the Data into the Apache Cassandra table `socialmedia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'data/socialMedia.csv'\n",
    "input_file = open(fileName, 'r')\n",
    "i = 1\n",
    "for line in input_file:\n",
    "    statusId = i\n",
    "    row = line.split(',')\n",
    "    \n",
    "    if row[0] != \"photo\" and row[0] != \"video\":\n",
    "        continue\n",
    "\n",
    "    query = \"INSERT INTO socialmedia (status_id, social_type, num_reactions, num_comments, num_shares,\\\n",
    "                               num_likes, num_loves, num_wows, num_hahas, num_sads, num_angrys)\"\n",
    "    query = query + \" VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "    session.execute(query, (statusId, row[0], int(row[1]), int(row[2]), int(row[3]), int(row[4]), int(row[5]), int(row[6]), int(row[7]), int(row[8]), int(row[9])))\n",
    "    i = i + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Apache Spark\n",
    "<img src=\"images/sparklogo.png\" width=\"150\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a spark session that is connected to DSE. From there load each table into a Spark Dataframe and take a count of the number of rows in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('demo').master(\"local\").getOrCreate()\n",
    "\n",
    "\n",
    "socialDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"socialmedia\", keyspace=\"accelerate\").load()\n",
    "\n",
    "print (\"Table Row Count: \")\n",
    "print (socialDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(socialDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When working with `STRING` data types you need to turn those `STRING` types into `FLOAT` types. Creating labels that K-MEANS and Apahce SPARK can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"social_type\", outputCol=\"label\", handleInvalid='keep')\n",
    "training = labelIndexer.fit(socialDF).transform(socialDF)\n",
    "\n",
    "showDF(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(training.select(\"social_type\",\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.groupBy('social_type').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize this data with a scatter plot \n",
    "### The x axis will be number of likes \n",
    "### The y axis will be number of comments\n",
    "### The color of the dot will be assigned based on its \"cluster\" Photo or Video\n",
    "\n",
    "Note: These attributes are what might be a strong attributes to finding clusters (Photo - Video)\n",
    "Note 1: Must move to a Pandas dataframe to do this visualization (be aware! This can't always be done as is, depends on your data size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smPanda = training.toPandas()\n",
    "smPanda.plot.scatter(x = 'num_likes', y = 'num_comments', c= 'label', figsize=(12,8), colormap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two clusters here Yellow = Video  and Purple = Pictures\n",
    "\n",
    "From what we can see from these two attributes Videos get less likes but more comments. Pictures get less comments but more likes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see if Kmeans can give us the same clustering\n",
    "\n",
    "## K-means clustering is a simple unsupervised learning algorithm that is used to solve clustering problems. Kmeans is very simple, but very powerful even on large datasets. It requires that all the input columns be vectorized. \n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-features.html#vectorassembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[ 'num_likes', 'num_comments'],\n",
    "    outputCol='features')\n",
    "\n",
    "trainingData = assembler.transform(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to set the K for KMeans which we will set at 2. One of the downsides of unsupervised learning is that we normally will not have clusteres predefinded (like we do in this case). Kmeans will happily split the data into as many clusters as you set. \n",
    "\n",
    "#### First we will generate the model and then make predictions based on that model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(trainingData)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(trainingData)\n",
    "\n",
    "showDF(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this case because we are actually preforming surpervised learnings (since we do have the cluster labels) we can do some comparisions to see if our predictions are correct. \n",
    "\n",
    "## In this case I am just taking a look at the counts for each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy('prediction').count().show()\n",
    "training.groupBy('social_type').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create another scatter plot to see if this lines up with our orignal scatter plot. \n",
    "\n",
    "## Everything is the same except now our dots will represent the color of the prediction (instead of the orginal cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df = predictions.toPandas()\n",
    "\n",
    "car_df.plot.scatter(x = 'num_likes', y = 'num_comments', c= 'prediction', figsize=(12,8), colormap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Videos are represented in yellow and pictures are represnted in purple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans struggles when you add many variables, so adding more variables is unlikely to help. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember Data Science and analytics is an iterative process! It's a science! Hypothesis, test, analysis, and loop again! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
